"""
Test Ä‘Æ¡n giáº£n cho AnthropicScraper - khÃ´ng cáº§n database hay config.
Chá»‰ test cÃ¡c chá»©c nÄƒng cÆ¡ báº£n: parse RSS, extract article ID, fetch content, convert HTML to markdown.

Script nÃ y hoÃ n toÃ n Ä‘á»™c láº­p, khÃ´ng import cÃ¡c module cÃ³ váº¥n Ä‘á» vá»›i config.
"""
import logging
import tempfile
from pathlib import Path
from urllib.parse import urlparse

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')

import feedparser
import requests
from docling.document_converter import DocumentConverter

# Copy cÃ¡c hÃ m utility tá»« AnthropicScraper Ä‘á»ƒ test Ä‘á»™c láº­p
def extract_article_id(url: str) -> str:
    """Extract unique article ID from URL"""
    parsed = urlparse(url)
    # Use path as ID, removing leading/trailing slashes
    article_id = parsed.path.strip("/")
    if not article_id:
        # Fallback to full URL if path is empty
        article_id = url
    return article_id

def parse_rss_feed(rss_url: str) -> feedparser.FeedParserDict | None:
    """Parse RSS feed using feedparser"""
    try:
        # For GitHub raw URLs, fetch content first to avoid Content-Type issues
        if "raw.githubusercontent.com" in rss_url:
            session = requests.Session()
            session.headers.update({
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            })
            response = session.get(rss_url, timeout=30)
            response.raise_for_status()
            # Parse from string content instead of URL
            feed = feedparser.parse(response.content)
        else:
            # For regular URLs, parse directly
            feed = feedparser.parse(rss_url)
        
        if feed.bozo and feed.bozo_exception:
            print(f"âŒ RSS feed parsing error: {feed.bozo_exception}")
            return None
        return feed
    except Exception as e:
        print(f"âŒ Failed to fetch RSS feed: {rss_url} - {e}")
        return None

def fetch_article_content(url: str) -> str | None:
    """Fetch full article content from URL"""
    try:
        session = requests.Session()
        session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })
        response = session.get(url, timeout=30)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"âŒ Failed to fetch article content: {url} - {e}")
        return None

def html_to_markdown(html_content: str) -> str | None:
    """Convert HTML content to markdown using docling"""
    # docling's DocumentConverter.convert() expects a file path or URL,
    # not a string. We need to write HTML to a temporary file first.
    with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp_file:
        try:
            tmp_file.write(html_content)
            tmp_file_path = tmp_file.name
            tmp_file.close()  # Close file so docling can read it
            
            converter = DocumentConverter()
            result = converter.convert(tmp_file_path)
            
            # Extract markdown from the result
            if hasattr(result, "document"):
                doc = result.document
            else:
                doc = result
            
            # Try to get markdown content
            if hasattr(doc, "export_to_markdown"):
                markdown = doc.export_to_markdown()
            elif hasattr(doc, "markdown"):
                markdown = doc.markdown
            elif hasattr(result, "export_to_markdown"):
                markdown = result.export_to_markdown()
            else:
                print("âš ï¸ Could not extract markdown from docling result")
                markdown = None
            
            return markdown
        except Exception as e:
            print(f"âŒ Failed to convert HTML to markdown: {e}")
            import traceback
            traceback.print_exc()
            return None
        finally:
            # Clean up temporary file
            try:
                Path(tmp_file_path).unlink(missing_ok=True)
            except Exception:
                pass

def test_extract_article_id():
    """Test hÃ m extract article ID tá»« URL"""
    print("ğŸ§ª Test: Extract Article ID tá»« URL")
    print("-" * 60)
    
    test_cases = [
        ("https://www.anthropic.com/news/claude-3-5-sonnet", "news/claude-3-5-sonnet"),
        ("https://www.anthropic.com/research/scaling-laws", "research/scaling-laws"),
        ("https://www.anthropic.com/news/", "news"),
        ("https://www.anthropic.com/", "https://www.anthropic.com/"),  # Fallback case
    ]
    
    for url, expected_id in test_cases:
        article_id = extract_article_id(url)
        print(f"âœ… URL: {url}")
        print(f"   Article ID: {article_id}")
        assert article_id == expected_id, f"Expected {expected_id}, got {article_id}"
    
    print("âœ… Test extract article ID: PASSED\n")

def test_parse_rss_feed():
    """Test parse RSS feed tá»« Anthropic blog"""
    print("ğŸ§ª Test: Parse RSS Feed tá»« Anthropic Blog")
    print("-" * 60)
    
    # Test vá»›i táº¥t cáº£ 3 RSS feeds
    rss_feeds = [
        ("Research", "https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_research.xml"),
        ("News", "https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_news.xml"),
        ("Engineering", "https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_engineering.xml"),
    ]
    
    for feed_name, rss_url in rss_feeds:
        print(f"\nğŸ“¡ Testing {feed_name} feed: {rss_url}")
        feed = parse_rss_feed(rss_url)
        
        if feed and hasattr(feed, 'entries'):
            print(f"âœ… Parse thÃ nh cÃ´ng!")
            print(f"   Sá»‘ bÃ i viáº¿t trong feed: {len(feed.entries)}")
            
            if feed.entries:
                # Láº¥y bÃ i viáº¿t Ä‘áº§u tiÃªn
                first_article = feed.entries[0]
                print(f"\nğŸ“„ BÃ i viáº¿t Ä‘áº§u tiÃªn:")
                print(f"   Title: {first_article.get('title', 'N/A')[:80]}...")
                print(f"   Link: {first_article.get('link', 'N/A')}")
                print(f"   Published: {first_article.get('published', 'N/A')}")
                
                # Test extract article ID
                article_id = extract_article_id(first_article.get('link', ''))
                print(f"   Article ID: {article_id}")
        else:
            print(f"âš ï¸ KhÃ´ng parse Ä‘Æ°á»£c RSS feed hoáº·c feed rá»—ng")
    
    print("\nâœ… Test parse RSS feed: COMPLETED\n")

def test_fetch_article_content():
    """Test fetch article content tá»« má»™t URL cá»¥ thá»ƒ"""
    print("ğŸ§ª Test: Fetch Article Content")
    print("-" * 60)
    
    # Láº¥y má»™t bÃ i viáº¿t tá»« RSS feed Ä‘á»ƒ test
    rss_url = "https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_research.xml"
    feed = parse_rss_feed(rss_url)
    
    if not feed or not hasattr(feed, 'entries') or not feed.entries:
        print("âš ï¸ KhÃ´ng thá»ƒ láº¥y RSS feed Ä‘á»ƒ test")
        return
    
    # Láº¥y bÃ i viáº¿t Ä‘áº§u tiÃªn
    test_article = feed.entries[0]
    test_url = test_article.get('link', '')
    
    if not test_url:
        print("âš ï¸ BÃ i viáº¿t khÃ´ng cÃ³ URL")
        return
    
    print(f"ğŸ“„ Äang fetch content tá»«: {test_url}")
    print(f"   Title: {test_article.get('title', 'N/A')}")
    
    content = fetch_article_content(test_url)
    
    if content:
        print(f"âœ… Fetch content thÃ nh cÃ´ng!")
        print(f"   Äá»™ dÃ i HTML: {len(content)} kÃ½ tá»±")
        print(f"   Preview: {content[:200]}...")
    else:
        print("âš ï¸ KhÃ´ng thá»ƒ fetch content")
    
    print("\nâœ… Test fetch article content: COMPLETED\n")

def test_html_to_markdown():
    """Test convert HTML to markdown báº±ng docling"""
    print("ğŸ§ª Test: Convert HTML to Markdown")
    print("-" * 60)
    
    # Test vá»›i HTML Ä‘Æ¡n giáº£n
    test_html = """
    <html>
        <body>
            <h1>Test Article</h1>
            <p>This is a <strong>test</strong> paragraph with <em>formatting</em>.</p>
            <ul>
                <li>Item 1</li>
                <li>Item 2</li>
            </ul>
            <a href="https://example.com">Link</a>
        </body>
    </html>
    """
    
    print("ğŸ“ Äang convert HTML sample...")
    markdown = html_to_markdown(test_html)
    
    if markdown:
        print(f"âœ… Convert thÃ nh cÃ´ng!")
        print(f"   Äá»™ dÃ i markdown: {len(markdown)} kÃ½ tá»±")
        print(f"   Preview:\n{markdown[:300]}")
    else:
        print("âš ï¸ KhÃ´ng thá»ƒ convert HTML to markdown")
        print("   (CÃ³ thá»ƒ do docling API khÃ¡c vá»›i expected)")
    
    print("\nâœ… Test HTML to markdown: COMPLETED\n")

def test_full_workflow():
    """Test workflow Ä‘áº§y Ä‘á»§: RSS -> Extract -> Fetch -> Convert"""
    print("ğŸ§ª Test: Full Workflow (RSS -> Extract -> Fetch -> Convert)")
    print("-" * 60)
    
    rss_url = "https://raw.githubusercontent.com/Olshansk/rss-feeds/main/feeds/feed_anthropic_research.xml"
    print(f"ğŸ“¡ Step 1: Parse RSS feed...")
    feed = parse_rss_feed(rss_url)
    
    if not feed or not hasattr(feed, 'entries') or not feed.entries:
        print("âš ï¸ KhÃ´ng thá»ƒ láº¥y RSS feed")
        return
    
    # Láº¥y bÃ i viáº¿t Ä‘áº§u tiÃªn
    article = feed.entries[0]
    article_url = article.get('link', '')
    article_title = article.get('title', 'N/A')
    
    if not article_url:
        print("âš ï¸ BÃ i viáº¿t khÃ´ng cÃ³ URL")
        return
    
    print(f"âœ… Step 1: Parse RSS thÃ nh cÃ´ng")
    print(f"   Article: {article_title}")
    print(f"   URL: {article_url}")
    
    # Extract article ID
    print(f"\nğŸ“ Step 2: Extract article ID...")
    article_id = extract_article_id(article_url)
    print(f"âœ… Step 2: Article ID = {article_id}")
    
    # Fetch content (skip náº¿u quÃ¡ lÃ¢u, chá»‰ test vá»›i summary)
    print(f"\nğŸŒ Step 3: Fetch article content...")
    print(f"   (Skipping Ä‘á»ƒ trÃ¡nh timeout - chá»‰ test vá»›i summary tá»« RSS)")
    summary = article.get('summary', '')
    if summary:
        print(f"âœ… Step 3: CÃ³ summary tá»« RSS feed")
        print(f"   Summary length: {len(summary)} kÃ½ tá»±")
    
    # Test convert vá»›i summary HTML náº¿u cÃ³
    if summary:
        print(f"\nğŸ”„ Step 4: Convert HTML to markdown...")
        markdown = html_to_markdown(summary)
        if markdown:
            print(f"âœ… Step 4: Convert thÃ nh cÃ´ng!")
            print(f"   Markdown length: {len(markdown)} kÃ½ tá»±")
        else:
            print(f"âš ï¸ Step 4: Convert tháº¥t báº¡i (cÃ³ thá»ƒ do docling API)")
    
    print("\nâœ… Test full workflow: COMPLETED\n")

def main():
    """Cháº¡y táº¥t cáº£ cÃ¡c test"""
    print("=" * 60)
    print("ğŸš€ Báº®T Äáº¦U TEST ANTHROPIC SCRAPER (ÄÆ¡n giáº£n)")
    print("=" * 60)
    print()
    
    try:
        # Test cÃ¡c chá»©c nÄƒng cÆ¡ báº£n
        test_extract_article_id()
        test_parse_rss_feed()
        test_fetch_article_content()
        test_html_to_markdown()
        test_full_workflow()
        
        print("=" * 60)
        print("âœ… Táº¤T Cáº¢ TEST HOÃ€N THÃ€NH!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Lá»–I: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

